defaults:
  - ../config/envs@_here_
  - ../config/deepspeed_zero@_here_
  - ../config/deepspeed_zero2@_here_
  - ../config/deepspeed_zero3@_here_
  - ../config/deepspeed_zero3_cpuoffload@_here_

hydra:
  run:
    dir: .
  output_subdir: null

# Use standard AgenticPipeline instead of DeepEyesPipeline
# pipeline_cls defaults to roll.pipeline.agentic.agentic_pipeline.AgenticPipeline

exp_name: "deepeyes_pipeline"
seed: 42
logging_dir: ./output/logs
output_dir: ./output
render_save_dir: ./output/render
system_envs:
  USE_MODELSCOPE: '1'

checkpoint_config:
  type: file_system
  output_dir: /data/cpfs_0/yuzhao/models/${exp_name}

# track_with: tensorboard
# tracker_kwargs:
#   log_dir: /data/oss_bucket_0/yuzhao/llm/tensorboard/roll_exp/deepeyes

offload_nccl: true

num_gpus_per_node: 8

max_steps: 1024
save_steps: 200
logging_steps: 1
eval_steps: 0
resume_from_checkpoint: false

rollout_batch_size: 2048 # 4096 # batch_size for dataloader * group_size
val_batch_size: 1024  # batch_size for dataloader
# prompt_length: 8192  # data.max_prompt_length in deepeyes: 8192
response_length: 1024  # data.max_response_length in deepeyes: 20480
sequence_length: 16384

reward_clip: 20
advantage_clip: 10.0
ppo_epochs: 1
adv_estimator: "grpo"
whiten_advantages: false
add_token_level_kl: false
use_kl_loss: false
init_kl_coef: 0.0
entropy_loss_coef: 0

pretrain: Qwen/Qwen2.5-VL-7B-Instruct

actor_train:
  system_envs:
    NVTE_FLASH_ATTN: '1'
    NVTE_FUSED_ATTN: '0'
    NVTE_UNFUSED_ATTN: '0'
  model_args:
    attn_implementation: fa2
    disable_gradient_checkpointing: false
    dtype: bf16
    model_type: ~
    freeze_module_prefix: "vision_model.blocks,vision_model.patch_embed"
  training_args:
    learning_rate: 1.0e-6
    lr_scheduler_type: constant
    weight_decay: 1.0e-2
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 512
    warmup_steps: 5
  strategy_args:
    strategy_name: megatron_train
    strategy_config:
      tensor_model_parallel_size: 2
      sequence_parallel: true
      context_parallel_size: 2
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      use_distributed_optimizer: true
      recompute_granularity: full
  device_mapping: list(range(0,16))
  infer_batch_size: 1
  offload_nccl: ${offload_nccl}
  use_dynamic_batching_in_train: true
  max_tokens_per_microbatch_in_train: 32768
  sequence_length_round_in_train: 8
  use_dynamic_batching_in_infer: true
  max_tokens_per_microbatch_in_infer: 32768
  sequence_length_round_in_infer: 8

actor_infer:
  model_args:
    disable_gradient_checkpointing: true
    dtype: bf16
  generating_args:
    max_new_tokens: ${response_length} # single-turn response length
    top_p: 1
    top_k: -1
    num_beams: 1
    temperature: 1.0
    num_return_sequences: 1
  strategy_args:
    strategy_name: vllm
    strategy_config:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.8
      block_size: 16
      # cache missing errors happen occasionally thus disable
      disable_mm_preprocessor_cache: true
      # enable_prefix_caching: false
      sleep_level: 2  # 2 will destroy model parameter and kv_cache after generate to save cpu memory, 1 will destroy kv_cache only.
  device_mapping: list(range(0,12))
  offload_nccl: ${offload_nccl}

reference:
  system_envs:
    NVTE_FLASH_ATTN: '1'
    NVTE_FUSED_ATTN: '0'
    NVTE_UNFUSED_ATTN: '0'
  model_args:
    attn_implementation: fa2
    disable_gradient_checkpointing: false
    dtype: bf16
    model_type: ~
  strategy_args:
    strategy_name: megatron_infer
    strategy_config:
      tensor_model_parallel_size: 2
      context_parallel_size: 2
      sequence_parallel: true
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
  device_mapping: list(range(0,16))
  infer_batch_size: 1
  offload_nccl: ${offload_nccl}
  use_dynamic_batching_in_train: true
  max_tokens_per_microbatch_in_train: 32768
  sequence_length_round_in_train: 8
  use_dynamic_batching_in_infer: true
  max_tokens_per_microbatch_in_infer: 32768
  sequence_length_round_in_infer: 8

# Reward cluster configuration for LLM-as-judge
# Uses InferWorker (default from AgenticConfig) for reward model inference
reward:
  name: deepeyes_reward
  # worker_cls defaults to InferWorker from AgenticConfig
  model_args:
    model_name_or_path: Qwen/Qwen2.5-72B-Instruct
    dtype: bf16
  generating_args:
    temperature: 0.3  # Lower temperature for stable judgment
  data_args:
    template: qwen2_5
  strategy_args:
    strategy_name: vllm
    strategy_config:
      tensor_parallel_size: 4
      gpu_memory_utilization: 0.8
      block_size: 16
      load_format: auto
  device_mapping: list(range(12,16))

max_actions_per_traj: 5
reward_normalization:
  grouping: traj_group_id # 可以tags(env_type)/traj_group_id(group)/batch(rollout_batch)... group_by计算reward/adv
  method: mean_std # asym_clip / identity / mean_std

custom_envs:
  deepeyes:
    env_type: deepeyes
    max_steps: ${max_actions_per_traj} # used in environment state manager to control the actual max actions executed per trajectory
    # used to curate llm prompt "max words", not used for rollout,
    # single_response_max_tokens in deepeyes: 10240
    max_tokens_per_step: ${response_length}
    env_manager_cls: roll.pipeline.agentic.env_manager.vl_traj_env_manager.VLTrajEnvManager
    use_thread_lock: true
    # max_env_step_concurrent: 256  # Control concurrent reward computation
    agent_system_template: ""
    pre_step_template: ""
    next_step_template: ""
    env_config:
      data_args:
        file_name: /data/oss_bucket_0/yuzhao/data/ChenShawn/DeepEyes-Datasets-47k/data_0.1.2_visual_toolbox_v2.parquet
        preprocessing_num_workers: 64
      max_steps: ${max_actions_per_traj}
      seed: ${seed}
      mode: train
      epoch: 0
      idx: 0
      # Reward weights for DeepEyes environment
      acc_weight: 0.8
      format_weight: 0.2
      tool_weight: 1.2
      enable_thinking: false

train_env_manager:
  max_env_num_per_worker: 32
  num_env_groups: 256
  # under the same group, the env config and env seed are ensured to be equal
  group_size: 8
  tags: [deepeyes]
  num_groups_partition:
    - 256
val_env_manager:
  max_env_num_per_worker: 32
  num_env_groups: ${val_batch_size}
  group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
  tags: [deepeyes]
  num_groups_partition:
    - ${val_batch_size}
